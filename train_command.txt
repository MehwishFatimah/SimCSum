sb deepspeed --num_gpus=4 /hits/basement/nlp/kolbertm/code/mtl_sum/run_summarization.py \
    --model_name_or_path facebook/mbart-large-cc25 \
    --src_lang en_XX \
    --tgt_lang de_DE \
    --do_train \
    --do_eval \
    --do_predict \
    --max_eval_samples 50 \
    --max_source_length 512 \
    --val_max_target_length 512 \
    --max_target_length 512 \
    --pad_to_max_length True \
    --ignore_pad_token_for_loss True \
    --num_train_epochs 30 \
    --train_file /hits/basement/nlp/fatimamh/wcs_hf/sim_sum/train.csv \
    --validation_file /hits/basement/nlp/fatimamh/wcs_hf/sim_sum/val.csv \
    --test_file /hits/basement/nlp/fatimamh/wcs_hf/sim_sum/test.csv \
    --output_dir /hits/basement/nlp/kolbertm/outputs/mtl_sum/ \
    --test_output_path /hits/basement/nlp/kolbertm/outputs/mtl_sum/predictions.csv \
    --load_best_model_at_end True \
    --greater_is_better True \
    --metric_for_best_model "eval_rouge1" \
    --sortish_sampler True \
    --group_by_length True \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate \
    --generation_max_length 200\
    --overwrite_output_dir \
    --logging_dir /hits/basement/nlp/kolbertm/outputs/mtl_sum/logging/ \
    --evaluation_strategy epoch \
    --logging_strategy steps \
    --logging_steps 1 \
    --logging_first_step True \
    --save_strategy epoch \
    --save_total_limit 5 \
    --optim "adamw_torch" \
    --fp16 \
    --fp16_full_eval \
    --report_to "wandb" \
    --deepspeed /hits/basement/nlp/kolbertm/code/mtl_sum/zero3_offload_config_accelerate.json 

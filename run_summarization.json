{
	"model_name_or_path": "facebook/mbart-large-cc25",
	"tokenizer_name": "facebook/mbart-large-cc25",
	"src_lang": "en_XX",
	"tgt_lang": "de_DE",
	"do_train": true,
	"do_eval": true,
    	"do_predict": true,
    	"max_source_length": 600,
	"val_max_target_length": 200,
	"max_target_length": 600,
	"pad_to_max_length": true,
	"ignore_pad_token_for_loss": true,
    	"num_train_epochs": 3,
    	"train_file": "/hits/basement/nlp/fatimamh/wcs_hf/sim_sum/train.csv",
    	"validation_file": "/hits/basement/nlp/fatimamh/wcs_hf/sim_sum/val.csv",
    	"test_file": "/hits/basement/nlp/fatimamh/wcs_hf/sim_sum/test.csv",
    	"output_dir": "/hits/basement/nlp/kolbertm/outputs/mtl_sum/",
    	"test_output_path": "/hits/basement/nlp/kolbertm/outputs/mtl_sum/predictions.csv",
    	"load_best_model_at_end": true,
    	"greater_is_better": true,
    	"metric_for_best_model": "eval_rouge1",
    	"sortish_sampler": true,
    	"group_by_length": true,
    	"per_device_train_batch_size": 2,
    	"per_device_eval_batch_size": 2,
    	"predict_with_generate": true,
    	"generation_max_length": 200,
    	"overwrite_output_dir": true,
    	"logging_dir": "/hits/basement/nlp/kolbertm/outputs/mtl_sum/logging/",
    	"evaluation_strategy": "steps",
    	"logging_strategy": "steps",
    	"save_strategy": "steps",
    	"save_steps": 100,
    	"logging_steps": 1,
    	"eval_steps": 100,
    	"logging_first_step": true,
    	"save_total_limit": 1,
	"optim": "adamw_torch",
	"fp16": true,
	"fp16_full_eval": true,
	"report_to": "wandb",
	"deepspeed": "zero3_offload_config_accelerate.json" 
}

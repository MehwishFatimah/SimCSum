{
	"model_name_or_path": "/hits/basement/nlp/kolbertm/outputs/mtl_sum/checkpoint-500",
	"tokenizer_name": "/hits/basement/nlp/kolbertm/outputs/mtl_sum/checkpoint-500",
	"src_lang": "en_XX",
	"tgt_lang": "de_DE",
    	"do_predict": true,
    	"max_source_length": 128,
	"val_max_target_length": 128,
	"max_target_length": 128,
	"pad_to_max_length": true,
	"ignore_pad_token_for_loss": true,
    	"test_file": "/hits/basement/nlp/fatimamh/wcs_hf/sim_sum/test.csv",
    	"output_dir": "/hits/basement/nlp/kolbertm/outputs/mtl_sum/",
    	"test_output_path": "/hits/basement/nlp/kolbertm/outputs/mtl_sum/predictions.csv",
    	"greater_is_better": true,
    	"metric_for_best_model": "eval_rouge1",
    	"sortish_sampler": true,
    	"group_by_length": true,
    	"per_device_eval_batch_size": 2,
    	"predict_with_generate": true,
    	"generation_max_length": 200,
    	"logging_dir": "/hits/basement/nlp/kolbertm/outputs/mtl_sum/logging/",
    	"logging_strategy": "steps",
    	"logging_steps": 1,
    	"logging_first_step": true,
    	"save_total_limit": 1,
	"optim": "adamw_torch",
	"fp16": true,
	"fp16_full_eval": true,
	"report_to": "wandb"
}
